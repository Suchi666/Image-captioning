{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4xX9TuQrglO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import csv\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngmWcokLO4k9",
        "outputId": "c21de2fc-7aa1-4bdd-ed98-b35b1919e837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zup5cpd0QVSj"
      },
      "outputs": [],
      "source": [
        "feature_size=1024\n",
        "hidden_size=feature_size\n",
        "learning_rate=3e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0feXoLiPUmcy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPIIfVv9qOKE"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size = 1024,train_CNN=False):\n",
        "      super(EncoderCNN, self).__init__()\n",
        "      self.train_CNN=False\n",
        "      # get the pretrained densenet model\n",
        "      self.densenet = models.densenet121(pretrained=True)\n",
        "      # replace the classifier with a fully connected embedding layer\n",
        "      self.densenet.classifier = nn.Linear(in_features=1024, out_features=1024)\n",
        "      # add another fully connected layer\n",
        "      self.embed = nn.Linear(in_features=1024, out_features=embed_size)\n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(p=0.5)\n",
        "      # activation layers\n",
        "      self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, images):\n",
        "        # get the embeddings from the densenet\n",
        "        densenet_outputs = self.dropout(self.prelu(self.densenet(images)))\n",
        "\n",
        "        # pass through the fully connected\n",
        "        embeddings = self.embed(densenet_outputs)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yah-ZIjIqpH2"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.lstm_cell = nn.LSTMCell(embed_size, hidden_size,num_layers)\n",
        "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
        "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
        "        self.dropout=nn.Dropout(0.5)\n",
        "        # activations\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "\n",
        "        # batch size\n",
        "        batch_size = features.size(0)\n",
        "\n",
        "        # init the hidden and cell states to zeros\n",
        "        hidden_state = torch.zeros((batch_size, self.hidden_size))\n",
        "        cell_state = torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "        # define the output tensor placeholder\n",
        "        outputs = torch.empty((batch_size, captions.shape[1], self.vocab_size))\n",
        "\n",
        "        # embed the captions\n",
        "        captions_embed = self.dropout(self.embed(captions))\n",
        "        # print(\"Captions_embed size :- \",captions_embed.shape)\n",
        "        # pass the caption word by word\n",
        "        for t in range(captions.size(1)):\n",
        "\n",
        "            # for the first time step the input is the feature vector\n",
        "            if t == 0:\n",
        "                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
        "\n",
        "            # for the 2nd+ time step, using teacher forcer\n",
        "            else:\n",
        "                # print(\"hidden_state size :- \",captions_embed[:, t, :].shape)\n",
        "                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
        "\n",
        "            # output of the attention mechanism\n",
        "            out = self.fc_out(hidden_state)\n",
        "\n",
        "            # build the output tensor\n",
        "            outputs[:, t, :] = out\n",
        "\n",
        "\n",
        "        return outputs\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfzYUenaRYeJ"
      },
      "outputs": [],
      "source": [
        "# dec_model=DecoderRNN(embed_size,hidden_size,vocab_size)\n",
        "# captions=caption.split()\n",
        "# indexes=[i for i in range(len(captions))]\n",
        "# vocab_dct={idx: word for idx,word in zip(indexes,captions)}\n",
        "# my_tensor = torch.tensor(indexes)\n",
        "# my_tensor = my_tensor.unsqueeze(0)\n",
        "# # indexes.size\n",
        "# my_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFsqNNLrVKws"
      },
      "outputs": [],
      "source": [
        "class TrainData(Dataset):\n",
        "    def __init__(self, img_path, caption_path):\n",
        "        # self.img_folder=image_path\n",
        "        file_list = os.listdir(img_path)\n",
        "        file_list = sorted(file_list, key=lambda x: int(x[6:-4]))\n",
        "        self.image_path = []\n",
        "        for file in file_list:\n",
        "          path=os.path.join(img_path,file)\n",
        "          self.image_path.append(path)\n",
        "\n",
        "        self.captions=[]\n",
        "        with open(caption_path, mode='r') as file:\n",
        "          reader = csv.reader(file)\n",
        "          for row in reader:\n",
        "            self.captions.append(row)\n",
        "        self.captions=self.captions[1:]\n",
        "        self.captions=[arr[2] for arr in self.captions]\n",
        "        # self.captions=self.captions[:10]\n",
        "\n",
        "        self.processor = transforms.Compose([\n",
        "          transforms.Resize((224, 224)),  # Resize image to match model input size\n",
        "          transforms.ToTensor(),           # Convert image to tensor\n",
        "          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image\n",
        "        ])\n",
        "\n",
        "        words = [word for string in self.captions for word in string.split()]\n",
        "        unique_words = list(set(words))\n",
        "        unique_words.sort()\n",
        "        unique_words.append('<START>')\n",
        "        unique_words.append('<EOS>')\n",
        "        self.vocabulary=unique_words.copy()\n",
        "        self.vocabulary={i:word for i,word in enumerate(self.vocabulary)}\n",
        "        with open('/content/drive/MyDrive/Colab Notebooks/Data/IC_dataset/vocabulary.json', 'w') as json_file:\n",
        "          json.dump(self.vocabulary, json_file)\n",
        "        self.encoder=EncoderCNN()\n",
        "        # self.vocabulary\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name=self.image_path[idx]\n",
        "        input_image = Image.open(img_name)\n",
        "        if input_image.mode != 'RGB':\n",
        "          input_image = input_image.convert('RGB')\n",
        "        input_tensor = self.processor(input_image)\n",
        "        input_batch = input_tensor.unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          output = self.encoder.forward(input_batch)\n",
        "\n",
        "        captions=self.captions[idx].split()\n",
        "\n",
        "        indexes={word:i for i,word in self.vocabulary.items()}\n",
        "        # print(indexes)\n",
        "        start_token=indexes['<START>']\n",
        "        caption_idx=[]\n",
        "        caption_idx.append(start_token)\n",
        "        word_idx_map=[indexes[word] for word in captions]\n",
        "        caption_idx=caption_idx+word_idx_map\n",
        "        caption_idx.append(indexes['<EOS>'])\n",
        "        token = torch.tensor(caption_idx)\n",
        "        token = token.unsqueeze(0)\n",
        "\n",
        "        return {\n",
        "            'feature':output,\n",
        "            'caption':token\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caption_path=\"/content/drive/MyDrive/Colab Notebooks/Data/IC_dataset/train.csv\"\n",
        "img_path=r\"/content/drive/MyDrive/Colab Notebooks/Data/IC_dataset/train\"\n",
        "train_data=TrainData(img_path,caption_path)\n",
        "voc=train_data.vocabulary\n",
        "# data=train_data.__getitem__(0)\n",
        "# data['caption']\n",
        "# voc\n",
        "data=train_data.__getitem__(87)\n",
        "# data_dict={}\n",
        "# for i,batch in enumerate(train_data):\n",
        "#   print(i ,\"  shapes :- \",batch['feature'][0].shape,\" \",batch['caption'][0].shape)\n",
        "#   output_list = batch['feature'][0].tolist()\n",
        "#   token_list = batch['caption'][0].tolist()\n",
        "#   # Create a dictionary\n",
        "#   data = {\n",
        "#     'feature': output_list,\n",
        "#     'caption': token_list\n",
        "#   }\n",
        "#   data_dict[i]=data\n",
        "#   # Save the dictionary to a JSON file\n",
        "#   if i!=0 and i%1000==0:\n",
        "#     with open(f'/content/drive/MyDrive/Colab Notebooks/Data/IC_dataset/train_data{}.json', 'w') as json_file:\n",
        "#       json.dump(data_dict, json_file)\n",
        "#     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdM2TRiYRvPy",
        "outputId": "9d9f9735-8bb8-4edd-b5ca-5e1228859cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 138MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dInsJFyPMTOa"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size = 1, shuffle = False)\n",
        "data_dict={}\n",
        "j=0\n",
        "for i,batch in enumerate(train_dataloader):\n",
        "  print(i ,\"  shapes :- \",batch['feature'][0].shape,\" \",batch['caption'][0].shape)\n",
        "  output_list = batch['feature'][0].tolist()\n",
        "  token_list = batch['caption'][0].tolist()\n",
        "  # Create a dictionary\n",
        "  data = {\n",
        "    'feature': output_list,\n",
        "    'caption': token_list\n",
        "  }\n",
        "  data_dict[i]=data\n",
        "  # Save the dictionary to a JSON file\n",
        "  if i!=0 and i%1000==0:\n",
        "    j=j+1\n",
        "    with open(f'/content/drive/MyDrive/Colab Notebooks/Data/IC_dataset/train_data_{i/1000}.json', 'w') as json_file:\n",
        "      json.dump(data_dict, json_file)\n",
        "    data_dict={}\n",
        "with open(f'/content/drive/MyDrive/Colab Notebooks/Data/IC_dataset/train_data_{j+1}.json', 'w') as json_file:\n",
        "  json.dump(data_dict, json_file)\n",
        "# data=train_data.__getitem__(0)\n",
        "# print(train_data.__getitem__(0)['feature'].shape[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "merged_data = {}\n",
        "# Loop through each JSON file\n",
        "for i in range(1, 7):\n",
        "    with open(f'/content/drive/MyDrive/Colab Notebooks/Data/IC_dataset/train_data_{i}.json', 'r') as file:\n",
        "        data = json.load(file)  # Load JSON content into a dictionary\n",
        "        merged_data.update(data)  # Merge the dictionaries\n",
        "print(len(merged_data.keys()))\n",
        "# Write the merged data into a new JSON file\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Data/IC_dataset/train_data.json', 'w') as outfile:\n",
        "    json.dump(merged_data, outfile, indent=4)  # Write merged data to the file with indentation"
      ],
      "metadata": {
        "id": "Oz7euEvXAcNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################  TRAIN LOOP  #########################\n",
        "model=DecoderRNN(feature_size,feature_size,len(train_data.vocabulary))\n",
        "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "losses = list()\n",
        "for batch in train_dataloader:\n",
        "    print(\"shapes :- \",batch['feature'][0].shape,\" \",batch['caption'][0].shape)\n",
        "    output = model.forward(batch['feature'][0],batch['caption'][0])\n",
        "    loss = criterion(output.view(-1, len(train_data.vocabulary)), batch['caption'][0].contiguous().view(-1))\n",
        "    losses.append(loss)\n",
        "    # optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "btMoZjIhOd8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFogI-abCHpe"
      },
      "outputs": [],
      "source": [
        "losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E8jh35jQXOQ"
      },
      "outputs": [],
      "source": [
        "##################################### TEST LOOP  #########################\n",
        "vocabulary=train_data.vocabulary\n",
        "processor = transforms.Compose([\n",
        "          transforms.Resize((224, 224)),  # Resize image to match model input size\n",
        "          transforms.ToTensor(),           # Convert image to tensor\n",
        "          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image\n",
        "        ])\n",
        "test_img_path=r\"/content/drive/MyDrive/Colab Notebooks/Data/images/train_1.jpg\"\n",
        "input_image = Image.open(test_img_path)\n",
        "encoder=EncoderCNN()\n",
        "input_tensor = processor(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    features = encoder.forward(input_batch)\n",
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0ef3IgsILgC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b62EelWCGIN9"
      },
      "outputs": [],
      "source": [
        "vocabulary={i:word for i,word in enumerate(vocabulary)}\n",
        "# model.predict(features,10, vocabulary)\n",
        "states=None\n",
        "hiddens=None\n",
        "# features.shape\n",
        "max_words=10\n",
        "embed_hidst=features.clone()\n",
        "output=[]\n",
        "for _ in range(max_words):\n",
        "  if hiddens==None:\n",
        "    hiddens, states = model.lstm_cell(features, states)\n",
        "  else:\n",
        "    hiddens, states = model.lstm_cell(embed_hidst, (hiddens,states))\n",
        "  output = model.fc_out(hiddens.unsqueeze(0))\n",
        "  max_index = torch.argmax(output)\n",
        "  print(max_index)\n",
        "  output.append(max_index)\n",
        "  max_index=max_index.unsqueeze (0)\n",
        "  embed_hidst=model.embed(max_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poHab515NEqF"
      },
      "outputs": [],
      "source": [
        "# output\n",
        "hiddens, states = model.lstm_cell(embed_hidst, (hiddens,states))\n",
        "output = model.fc_out(hiddens.unsqueeze (0))\n",
        "max_index = torch.argmax(output)\n",
        "max_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po7zK1H3py1D"
      },
      "outputs": [],
      "source": [
        "def caption_image(self, image, vocabulary, max_length=50): result_caption = []\n",
        "with torch.no_grad():\n",
        "x = self.encoderCNN(image).unsqueeze (0)\n",
        "states = None\n",
        "for in range(max_length):\n",
        "hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "output = self.decoderRNN.linear(hiddens.unsqueeze (0)) predicted = output.argmax(1)\n",
        "result_caption.append(predicted.item()) x = self.decoderRNN.embed(predicted).unsqueeze (0)\n",
        "if vocabulary.itos [predicted.item()] = \"<EOS>\":\n",
        "break\n",
        "return [vocabulary.itos [idx] for idx in result_caption]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kj8QW0YtH-KB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}